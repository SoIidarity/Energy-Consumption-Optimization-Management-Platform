{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flask Playground\n",
    "\n",
    "### This notebook containers Python code used to pull information from Excel spreadsheets and load it into an Influxdb. Computation is done over the data set and stored within a mongodb. This code would in the future be moved into the flask API but in order to do this the full Selenium python scraper would need to be built out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "from pymongo import MongoClient\n",
    "from bson.objectid import ObjectId\n",
    "from influxdb import InfluxDBClient\n",
    "from influxdb import DataFrameClient\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "from difflib import get_close_matches\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongoClient = MongoClient('95.179.179.222',27017)\n",
    "db = mongoClient.witsECOMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure InfluxDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "databaseName = \"timeseriesdata\"\n",
    "influxClient = DataFrameClient(\"95.179.179.222\", 8086, \"root\", \"root\", database=databaseName)\n",
    "influxClient.create_database(databaseName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# influxClient.drop_database(databaseName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in time Series from excel spreadsheet\n",
    "Note: this process can take a long time (2 to 3 minutes!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spreadsheet = pd.read_excel('/home/chris/Desktop/Wits Data/WITS ecWIN7 Data/WITS ecWIN7 Data 2018.xlsx',\n",
    "                            sheet_name='WITS ecWIN7 Data 2018',\n",
    "                            index_col=\"DateTime\"\n",
    "                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in building names and geojson information\n",
    "These files were manually created by generating geojson from http://geojson.io/ and identifying the corisponding building names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buildingSensorNames = spreadsheet.columns\n",
    "print(buildingSensorNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadedGeoJson = {}\n",
    "with open('../../Assets/geojson/witsMainCampusGeojson.json') as json_data:\n",
    "    loadedGeoJson = json.load(json_data)\n",
    "    print(loadedGeoJson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buildingNamesStore = db.buildingNames\n",
    "#Itterate over all buildings defined in the geojson and for each find the most likely sensor from the excel spreadhseet\n",
    "#this is effectivly preforming a join between the two sets, based off weak links (the names)\n",
    "for index, feature in enumerate(loadedGeoJson[\"features\"]):\n",
    "    buildingName = feature[\"properties\"][\"buildingName\"]\n",
    "    #Find the most likly sensor for that particular building\n",
    "    mostLiklySensor = get_close_matches(buildingName + \"_kVA\", buildingSensorNames)[0]\n",
    "    dataFrame = spreadsheet[mostLiklySensor].to_frame()\n",
    "    influxClient.write_points(dataFrame, databaseName,{'buildingNumber': index, 'buildingName': buildingName, 'sensorName': mostLiklySensor})\n",
    "    print(buildingName + \"->\" + mostLiklySensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preform Queries to load agrigated data into the mongoDB.\n",
    "\n",
    "These queries MUST pull from the influxdb as this is where all the time series data is stored. As new records are added from Selenium (or directly from tnew sensors) they will be added to the Influx DB. this querying and updating mongo process occures every 30 minutes to generate \"fresh\" graphs.\n",
    "\n",
    "First, we need to make the database store for each building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buildingInformationStore = db.buildingInformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, itterate over all the buildings that have been identified in the geojson and query for values for that building from influxdb for the predefined length of time. For each element, influx db is queried and then the output is computed over to generate averages per day, week and year. The maximum building week is also extracted. This value is used later to generate the heat map.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buildingNamesStore = db.buildingNames\n",
    "startTime = \"2018-08-19T00:00:00+00:00\"\n",
    "buildingArray = []\n",
    "\n",
    "#Define a structure used in the creation of tables. itterate over each to define the graphs to draw\n",
    "plotsToDraw = {\n",
    "    'Day':{\n",
    "        \"resampleLength\":'1H',\n",
    "        \"windowLength\": 2*24, #1 day window with 2 samples per hour\n",
    "        \"moduloSize\": 24\n",
    "        },\n",
    "    'Week':{\n",
    "        \"resampleLength\":'1D',\n",
    "        \"windowLength\":2*24*7, # 1 week window with 2 samples per hour\n",
    "        \"moduloSize\": 7\n",
    "        },\n",
    "    'Year':{\n",
    "        \"resampleLength\":'1M',\n",
    "        \"windowLength\":2*24*30*8, #1 year window with 2 samples per hour\n",
    "        \"moduloSize\": 8\n",
    "        },\n",
    "}\n",
    "\n",
    "for index, feature in enumerate(loadedGeoJson[\"features\"]):\n",
    "    buildingName = feature[\"properties\"][\"buildingName\"]\n",
    "    #This blob is in accordance with the datastructure defined by swagger\n",
    "    buildingBlob = {\n",
    "        \"BuildingId\": index,\n",
    "        \"BuildingRank\": 0,\n",
    "        \"BuildingName\": buildingName,\n",
    "        \"BensorName\": \"SENSOR\",\n",
    "        \"ChartInformation\": {\n",
    "            \"DayInformation\": {\n",
    "                \"LastDay\": [],\n",
    "                \"AverageDay\": [],\n",
    "                \"LastDayAverage\":0,\n",
    "                \"MaximumDay\":0\n",
    "            },\n",
    "            \"WeekInformation\": {\n",
    "                \"LastWeek\": [],\n",
    "                \"AverageWeek\":[],\n",
    "                \"LastWeekAverage\": 0,\n",
    "                \"MaximumWeek\":0\n",
    "            },\n",
    "            \"YearInformation\": {          \n",
    "                \"LastYear\": [],\n",
    "                \"AverageYear\":[],\n",
    "                \"LastYearAverage\":0,\n",
    "                \"MaximumYear\":0\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "    for plotType in plotsToDraw:\n",
    "        #We want to get all the time series data for that particular building. The start time would be removed if live\n",
    "        #data was added to the data set as we would want to query results from the current time back to the begining of the set\n",
    "        query = \"SELECT * FROM timeseriesdata WHERE buildingName='{}' AND time< '{}'\".format(buildingName, startTime)\n",
    "        queryResults = influxClient.query(query)\n",
    "        #We need to extract the time series data from the influx query results\n",
    "        results = queryResults[\"timeseriesdata\"][queryResults[\"timeseriesdata\"].keys()[0]]\n",
    "        #For the resampled region(to calculculate the last day/week/year) we dont need the full set so sub\n",
    "        #set using the tail (to get last x entries)\n",
    "        resampledResults = results.tail(plotsToDraw[plotType]['windowLength']).resample(plotsToDraw[plotType]['resampleLength'], label='right').sum()\n",
    "        #we can calculate the average overthe last period resampled region\n",
    "        buildingBlob[\"ChartInformation\"][plotType+\"Information\"][\"Last\" + plotType + \"Average\"]=resampledResults.mean()\n",
    "        \n",
    "        #Next, we want to convert the results into a format that mongo can accept. we cast it to a dict\n",
    "        #and then itterate over all results\n",
    "        resultsDict = resampledResults.to_dict()\n",
    "        formattedDict = {} \n",
    "        for result in resultsDict:\n",
    "            formattedDict[str(result)[0:19]] = resultsDict[result]\n",
    "        buildingBlob[\"ChartInformation\"][plotType+\"Information\"][\"Last\" + plotType] = formattedDict\n",
    "        \n",
    "        #The last step is to calculate the period average over all samples(eg the average week, over all data points)\n",
    "        \n",
    "        resampledResults_totalSet = results.resample(plotsToDraw[plotType]['resampleLength'], label='right').sum()\n",
    "\n",
    "        buildingBlob[\"ChartInformation\"][plotType+\"Information\"][\"Maximum\"+plotType]=np.max(resampledResults_totalSet)\n",
    "        resampledResults_totalSetDict = resampledResults_totalSet.to_dict()\n",
    "        \n",
    "        timeSeriesValues =  [[] for _ in range(plotsToDraw[plotType]['moduloSize'])]\n",
    "        for index, result in enumerate(resampledResults_totalSetDict):\n",
    "            relativeIndex = index % plotsToDraw[plotType]['moduloSize']\n",
    "            timeSeriesValues[relativeIndex].append(resampledResults_totalSetDict[result])\n",
    "        \n",
    "        finalizedAverage = []\n",
    "        for result in timeSeriesValues:\n",
    "            finalizedAverage.append(np.array(result).mean())\n",
    "  \n",
    "        #In order to plot these values correctly, they need to be bound off the last week key values        \n",
    "        averageResultsDict = {}\n",
    "        for index, key in enumerate(formattedDict):\n",
    "            averageResultsDict[key] = finalizedAverage[index]\n",
    "        buildingBlob[\"ChartInformation\"][plotType+\"Information\"][\"Average\" + plotType]=averageResultsDict\n",
    "\n",
    "        \n",
    "    buildingArray.append(buildingBlob)\n",
    "    print (\"Building: {} Added with id {}\".format(buildingName, len(buildingArray)))\n",
    "\n",
    "# Last step is to order the buildings to extract position of building relative to others\n",
    "sortedArray = sorted(buildingArray, key=lambda k: k['ChartInformation']['YearInformation']['LastYearAverage'], reverse=True) \n",
    "\n",
    "\n",
    "for index, element in enumerate(sortedArray):\n",
    "    buildingId = element[\"BuildingId\"]\n",
    "    buildingName = element[\"BuildingName\"]\n",
    "    element[\"BuildingRank\"] = index\n",
    "    buildingInformation_id = buildingInformationStore.insert_one(element).inserted_id\n",
    "    buildingNamesStore.insert_one({\"buildingName\": buildingName, \"BuildingId\": buildingId}).inserted_id\n",
    "    print(\"record added to db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the university wide metrics\n",
    "We want to be able to view the relative values for each set. These are averages over all time for all buildings. We already know what the average for each building is from before so we just need to find the average over all buildings for those particular sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buildingInformation = list(buildingInformationStore.find({}))\n",
    "\n",
    "campusInformation = {\n",
    "    \"AveragePastDay\": {},\n",
    "    \"AveragePastWeek\": {},\n",
    "    \"AveragePastYear\": {},\n",
    "    \"Maximums\": {\n",
    "        \"Day\": 0,\n",
    "        \"Week\": 0,\n",
    "        \"Year\": 0\n",
    "    },\n",
    "    \"MaximumLastPeriodAverage\": {\n",
    "        \"Day\": 0,\n",
    "        \"Week\": 0,\n",
    "        \"Year\": 0\n",
    "    }\n",
    "}\n",
    "#Itterate through all the plots that need averages\n",
    "for plotType in plotsToDraw:\n",
    "    typeAverageArray = []\n",
    "    \n",
    "    #For each building, extract their relevent time series values to sum over to get averages\n",
    "    #Each position in the timeSeriesValues array corisponds to a day(it is an array of 7 for a week, for example)\n",
    "    timeSeriesValues =  [[] for _ in range(plotsToDraw[plotType]['moduloSize'])]\n",
    "    #itterate over all buildings\n",
    "    for building in buildingInformation:\n",
    "        spesificBuildingArray = building[\"ChartInformation\"][plotType+\"Information\"][\"Average\" + plotType].values()\n",
    "        for index, value in enumerate(spesificBuildingArray):\n",
    "            timeSeriesValues[index].append(value)\n",
    "    \n",
    "    #calculate each everage and assign keys as time stamp values    \n",
    "    for index, key in enumerate(building[\"ChartInformation\"][plotType+\"Information\"][\"Average\" + plotType].keys()):\n",
    "        numpyArray = np.array(timeSeriesValues[index])\n",
    "        campusInformation[\"AveragePast\"+plotType][key] = numpyArray.mean()\n",
    "#         print(numpyArray.mean())\n",
    "    \n",
    "#         campusInformation[\"Maximums\"][plotType] = np.max(numpyArray)\n",
    "#         print(campusInformation[\"AveragePast\"+plotType][key])\n",
    "    \n",
    "    #Find maximum and MaximumLastPeriodAverage for each time period and asign it to struct\n",
    "    for building in buildingInformation:\n",
    "        buildingMax = building[\"ChartInformation\"][plotType+\"Information\"][\"Maximum\"+plotType]\n",
    "        if buildingMax > campusInformation[\"Maximums\"][plotType]:\n",
    "            campusInformation[\"Maximums\"][plotType] = buildingMax\n",
    "        buildingAverage = building[\"ChartInformation\"][plotType+\"Information\"][\"Last\"+plotType+\"Average\"]\n",
    "        if buildingAverage > campusInformation[\"MaximumLastPeriodAverage\"][plotType]:\n",
    "            campusInformation[\"MaximumLastPeriodAverage\"][plotType] = buildingAverage\n",
    "print(campusInformation)\n",
    "campusInfoStore = db.campusInfo\n",
    "campusInfo_id = campusInfoStore.insert_one(campusInformation).inserted_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate geojson for each location\n",
    "Colours need to be assigned based off a buildings magnitude. \n",
    "First, define a set of functions to convert a max/min/value into a RGB value. This value is used to define the colour on the final heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb(minimum, maximum, value):\n",
    "    minimum, maximum = float(minimum), float(maximum)\n",
    "    ratio = 2 * (value-minimum) / (maximum - minimum)\n",
    "    b = int(max(0, 255*(1 - ratio)))\n",
    "    r = int(max(0, 255*(ratio - 1)))\n",
    "    g = 255 - b - r\n",
    "    return r, g, b\n",
    "\n",
    "def getHex(minimum,maximum,value):\n",
    "    return '#%02x%02x%02x' % rgb(minimum,maximum,value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getHex(0,10,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we itterate over the geojson set and update the square heat based of the set maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # We want to gives the colours for the heatmap to show the consumption averages & maximums over a day, week and year\n",
    "groupings = [\"Average\",\"Maximum\"]\n",
    "for group in groupings:\n",
    "    for plotType in plotsToDraw:\n",
    "        for index, feature in enumerate(loadedGeoJson[\"features\"]):\n",
    "            buildingName = feature[\"properties\"][\"buildingName\"]\n",
    "            if group ==\"Average\":\n",
    "                value = buildingInformationStore.find_one({\"BuildingName\": buildingName})['ChartInformation'][plotType + 'Information'][\"Last\" +plotType + group]\n",
    "                maximum = campusInformation[\"MaximumLastPeriodAverage\"][plotType]\n",
    "            else:\n",
    "                maximum = campusInformation[\"Maximums\"][plotType]\n",
    "                value = buildingInformationStore.find_one({\"BuildingName\": buildingName})['ChartInformation'][plotType + 'Information'][group + plotType]\n",
    "            newColour = getHex(0,\n",
    "                               maximum,\n",
    "                               value)\n",
    "            print(maximum,value,newColour)\n",
    "#             print(buildingInformationStore.find_one({\"BuildingName\": buildingName})['ChartInformation'][plotType + 'Information']['Last'+ plotType + group])\n",
    "            loadedGeoJson[\"features\"][index][\"properties\"][plotType + \"Style_\" + group][\"fillColor\"] = newColour\n",
    "            loadedGeoJson[\"features\"][index][\"properties\"][\"buildingId\"] = index #set the index of the building based off the previous indecies\n",
    "print(loadedGeoJson)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert the geojson into the database\n",
    "Now that the heatmap colours have been calculated, we can incert it into the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geojsonStore = db.geojson\n",
    "geojson_id = geojsonStore.insert_one(loadedGeoJson).inserted_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadedCampusInfo = {}\n",
    "with open('../../Assets/otherinformation/campusInfomation.json') as json_data:\n",
    "    loadedCampusInfo = json.load(json_data)\n",
    "    print(loadedCampusInfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadedGeoJson = {}\n",
    "with open('../../Assets/geojson/witsMainCampusGeojson.json') as json_data:\n",
    "    loadedGeoJson = json.load(json_data)\n",
    "    print(loadedGeoJson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BuildingChartInformation = {}\n",
    "with open('../../Assets/ChartInformation/BuildingChartInformation.json') as json_data:\n",
    "    BuildingChartInformation = json.load(json_data)\n",
    "    print(BuildingChartInformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buildingInformationStore = db.buildingInformation\n",
    "buildingInformation_id = buildingInformationStore.insert_one(BuildingChartInformation).inserted_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(buildingInformationStore.find({\"buildingId\": \"0\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Attachments",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
